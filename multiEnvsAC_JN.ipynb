{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e745f5",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c529ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np, random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from math import sin,pi\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from gym import spaces\n",
    "import collections\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e872044",
   "metadata": {
    "id": "7e872044"
   },
   "source": [
    "# Create Environment #\n",
    "\n",
    "### stockMEnv ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20a1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "run stockMarketEnvironmentREDUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8fa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = stockMEnv()\n",
    "observation = env.reset()\n",
    "#print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5087b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info, cash = env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecc640",
   "metadata": {},
   "source": [
    "# Create REINFORCE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3033bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        batchS = tf.shape(x)[0]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        positions = tf.expand_dims(positions, axis = 0)\n",
    "        positions = tf.repeat(positions, batchS, axis=0)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2a1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = tf.keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73aa757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lapan recomenda ter o Actor e o Critic na mesma rede para partilharem low-level features. O valor do critic é como adicionar uma\n",
    "#baseline que depende do estado(V(s)) para estabilizar o gradiente. No reinforce fizemos isso na loss com um valor igual para qualquer\n",
    "#caso. O melhoramente aqui é depender de cada estado\n",
    "def AC(obs_shape,hidden_size,actions_n,head_size=64,num_heads=2,ff_dim=2,num_transformer_blocks=2,dropout=0.4):\n",
    "    #reshape = tf.keras.layers.Reshape((obs_shape[0]*obs_shape[1]))\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "    dense1 = tf.keras.layers.Dense(units=hidden_size, activation='relu')\n",
    "    dense2 = tf.keras.layers.Dense(units=hidden_size, activation='relu')\n",
    "    dense3 = tf.keras.layers.Dense(units=hidden_size, activation='relu')\n",
    "    dense_action = tf.keras.layers.Dense(units= actions_n, activation='softmax') \n",
    "    dense_value = tf.keras.layers.Dense(units= 1, activation='linear')\n",
    "    input_model = tf.keras.Input(shape=obs_shape)#pq espaço de obs é 3x90, de ser 90 time steps que olhamos para trás em cada decisão\n",
    "                                        # não interessa o batch size é como se fosse aplicado a um unico exemplo\n",
    "    x = input_model\n",
    "    x = PositionEmbedding(maxlen=env.number_past_steps+1, embed_dim=observation.shape[1])(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = flatten(x)\n",
    "    x = dense1(x)  \n",
    "    x = dense2(x)\n",
    "    x = dense3(x)\n",
    "    action = dense_action(x)\n",
    "    value = dense_value(x)\n",
    "    return tf.keras.Model(inputs=input_model,outputs=[action,value])\n",
    "\n",
    "\n",
    "hidden_size=100  \n",
    "gamma = 0.99\n",
    "ac = AC(observation.shape,hidden_size, env.nActions)\n",
    "AC_net_weights_file = 'AC_REDUX_MW.h5'\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.000006, epsilon=0.001)#ter cuidado a escolher lr, loss tende a explodir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353f2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 61, 2)]      0           []                               \n",
      "                                                                                                  \n",
      " position_embedding (PositionEm  (None, 61, 2)       122         ['input_1[0][0]']                \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 61, 2)       1410        ['position_embedding[0][0]',     \n",
      " dAttention)                                                      'position_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 61, 2)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 61, 2)       4           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 61, 2)       0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'position_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 61, 2)        6           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 61, 2)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 61, 2)        6           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 61, 2)       4           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 61, 2)       0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 61, 2)       1410        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 61, 2)        0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 61, 2)       4           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 61, 2)       0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 61, 2)        6           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 61, 2)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 61, 2)        6           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 61, 2)       4           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 61, 2)       0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 122)          0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          12300       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            303         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            101         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35,886\n",
      "Trainable params: 35,886\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ac.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d08b9",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba7b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "      def __init__(self,env):\n",
    "        self.env = env\n",
    "        self._reset()\n",
    "        self.render = False\n",
    "        \n",
    "      def _reset(self):\n",
    "        self.obs = env.reset()\n",
    "        self.render = False\n",
    "        \n",
    "      def play_step(self,state):\n",
    "        prob = reinforce(np.array([state])) #obter as prob sobre as ações do modelo\n",
    "        prob = prob[0]\n",
    "        action = np.random.choice(a=[i for i in range(len(prob))],p=prob)\n",
    "        #action = int(action.numpy()[0])\n",
    "        next_state, reward, done, cash, _ = self.env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            if self.render:\n",
    "                self.env.render()\n",
    "            self._reset()\n",
    "        return reward,next_state,action,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf39ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiEnvAgent:\n",
    "    def __init__(self,envs):\n",
    "        self.envs = envs \n",
    "        self.numEnvs = len(envs)\n",
    "        #self.aux_shape = (self.numenvs,) + self.envs[0].observation_space.shape\n",
    "        self.render = False\n",
    "        self.rewards = np.zeros(self.numEnvs)\n",
    "        self.obs = np.array([env.reset() for env in self.envs]) \n",
    "    \n",
    "    def _reset(self):\n",
    "        self.obs = np.array([env.reset() for env in self.envs]) \n",
    "        self.render = False\n",
    "    \n",
    "    def play_step(self,states):\n",
    "        \"\"\"states is an array with state for each env[i] i in range(numEnvs)\"\"\"\n",
    "        probs,values = ac(states) #obter as prob sobre as ações do modelo sobre cada env, shape=(num_envs,num_actions)\n",
    "        #pdb.set_trace()\n",
    "        probs = np.array(probs)\n",
    "        values = np.reshape(values,(values.shape[0],))#(20,1)->(20,)\n",
    "        actions = np.array([np.random.choice(a=[i for i in range(len(prob))],p=prob) for prob in probs])\n",
    "        actions = actions.astype(np.int32)\n",
    "        \n",
    "        next_states = self.obs.copy()\n",
    "        rewards = np.zeros(self.numEnvs)\n",
    "        for i in range(numEnvs):\n",
    "            next_states[i], rewards[i], done, cash, _ = envs[i].step(actions[i])\n",
    "        \n",
    "        if done: # done ocorre em simultâneo (neste env) para todos os envs\n",
    "            if self.render:\n",
    "                self.envs[0].render()\n",
    "            self._reset()   \n",
    "        return rewards,next_states,actions,values,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da4690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numEnvs =20\n",
    "envs = [stockMEnv() for i in range(numEnvs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adeb1b",
   "metadata": {},
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6906ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddb658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ac_multiEnvs(states, rewards, actions,values):\n",
    "    \"\"\" o número de episódios é igual ao número de envs\n",
    "      states, rewards, actions são matrizes com o numero de colunas igual ao \n",
    "      comprimento dos episodios e o numero de linhas igual ao numero de episodios\"\"\"\n",
    "    sum_reward = np.zeros(rewards.shape[1])\n",
    "    discnt_rewards = np.zeros((rewards.shape[0],rewards.shape[1]))\n",
    "    rewards = np.flip(rewards, axis=1)\n",
    "    for i in range(rewards.shape[1]):# vamos neste loop calcular a expected cumulative reward\n",
    "        sum_reward = rewards[i] + gamma*sum_reward\n",
    "        discnt_rewards[i] = sum_reward\n",
    "    discnt_rewards = np.flip(discnt_rewards, axis=-1)  \n",
    "        \n",
    "    states = np.reshape(states, (-1, states.shape[-2], states.shape[-1]))\n",
    "    discnt_rewards = np.reshape(discnt_rewards,-1)\n",
    "    actions = np.reshape(actions,-1)\n",
    "    values = np.reshape(values,-1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs,_ = ac(states)\n",
    "        probs = tf.gather(probs,actions,axis=1, batch_dims=1) \n",
    "        #values = tf.gather(values1,values,axis=1, batch_dims=1)\n",
    "        loss = -tf.math.log(probs)*(discnt_rewards-values)+(discnt_rewards-values)+0.001*probs*tf.math.log(probs)\n",
    "        pdb.set_trace()\n",
    "        #loss = policy loss+critic loss onde policy loss é igual à do reinforce com baseline dependente do estado e a do critic\n",
    "        #é a diferença entre V(s') e V(s). Adicionamos também um entropy bonus para melhorar a exploração regulado por um\n",
    "        #parametro beta=0.001\n",
    "    grads = tape.gradient(loss, ac.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, ac.trainable_variables))\n",
    "    ac.save_weights(AC_net_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45922c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_demo_episode(env, single_env_agent):\n",
    "    done = False\n",
    "    next_state = env.reset()\n",
    "    while not done:\n",
    "        reward,next_state,action,done= single_env_agent.play_step(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64f222",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee611a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 500\n",
    "epoch = 0\n",
    "SHOW_LOG_FREQ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b79d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 "
     ]
    }
   ],
   "source": [
    "# multiEnvsAgent train\n",
    "#reinforce.load_weights(REINFORCE_net_weights_file)\n",
    "magent = multiEnvAgent(envs)\n",
    "single_env_agent = Agent(env)\n",
    "lenEpisode = env.lenEpisode - env.number_past_steps + 1 #141\n",
    "numActions = 19\n",
    "EPOCH_NUM = 500\n",
    "epoch = 0\n",
    "\n",
    "# arrays onde  a primeira dimensão é o comprimento de cada episódio \n",
    "rewards = np.zeros((lenEpisode, magent.numEnvs))\n",
    "states = np.zeros((lenEpisode, magent.numEnvs,envs[0].observation_space.shape[0],envs[0].observation_space.shape[1]))\n",
    "actions =   np.zeros((lenEpisode, magent.numEnvs),dtype=np.int32)  \n",
    "values = np.zeros((lenEpisode, magent.numEnvs),dtype=np.int32) \n",
    "while EPOCH_NUM>epoch:\n",
    "    epoch+=1\n",
    "    done = False\n",
    "    state = np.array([magent.envs[i].reset() for i in range(magent.numEnvs)])\n",
    "    states[0] = state #(141,20,61,2)\n",
    "    #rewards = np.zeros(agent.numEnvs)\n",
    "    total_reward = 0\n",
    "    total_reward_list = []\n",
    "    print(epoch, end=' ')\n",
    "    if (epoch+1) % SHOW_LOG_FREQ == 0:\n",
    "        #print('\\n'+'\\t'.join(map(str, [epoch+1,np.mean(total_rewards)])))\n",
    "        total_rewards_list = []\n",
    "        magent.render = True\n",
    "        play_demo_episode(env, single_env_agent)\n",
    "        single_env_agent.render = False\n",
    "        continue\n",
    "    time =0\n",
    "    while not done:\n",
    "        # states[time +1],rewards[time+1],done,_ = agent.play_step(state) # ,actions[time]\n",
    "        rewards[time +1],states[time +1],actions[time +1],values[time +1],done = magent.play_step(state)\n",
    "        state = states[time +1]\n",
    "        time = time + 1\n",
    "        #for i in range(numEnvs):\n",
    "        #    rewards[i,time.append(reward[i])\n",
    "        #    states.append(state[i])\n",
    "        #    actions.append(action[i])\n",
    "        \n",
    "        #total_reward += np.sum(reward)\n",
    "        if done:\n",
    "            train_ac_multiEnvs(states, rewards, actions,values)\n",
    "            \"\"\"\n",
    "            rewards = np.zeros((lenEpisode, magent.numEnvs))\n",
    "            states = np.zeros((lenEpisode, magent.numEnvs,envs[0].observation_space.shape[0],envs[0].observation_space.shape[1]))\n",
    "            actions =   np.zeros((lenEpisode, magent.numEnvs),dtype=np.int32)  \n",
    "            values = np.zeros((lenEpisode, magent.numEnvs),dtype=np.int32) \n",
    "            #total_reward_list.append(total_reward)\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e700417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2350578",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1.e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5e61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
